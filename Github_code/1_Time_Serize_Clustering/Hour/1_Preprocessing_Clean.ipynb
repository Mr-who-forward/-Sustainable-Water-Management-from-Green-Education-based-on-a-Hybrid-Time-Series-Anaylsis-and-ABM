{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cleaning_for_one_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('demo_uncleaned.csv')\n",
    "\n",
    "# Step 1: Handle Missing Values\n",
    "# Reconfirm if there are any missing values in the original dataset and add those rows to rows_to_remove\n",
    "data_cleaned = data.copy()\n",
    "rows_to_remove = set(data_cleaned[data_cleaned.isna().any(axis=1)].index)\n",
    "missing_values_indices = list(rows_to_remove)\n",
    "missing_values_count = len(missing_values_indices)\n",
    "\n",
    "# Step 2: Remove Rows with Negative Values\n",
    "# Assuming that negative values are not valid for this dataset, add rows with negative values to rows_to_remove\n",
    "for col in data_cleaned.select_dtypes(include='number').columns:\n",
    "    negative_indices = data_cleaned[data_cleaned[col] < 0].index\n",
    "    rows_to_remove.update(negative_indices)\n",
    "negative_values_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices]\n",
    "negative_values_count = len(negative_values_indices)\n",
    "\n",
    "# Step 3: Handle Outliers by Removing Rows with Outliers (Based on Row Statistics)\n",
    "# Define outliers as values beyond 3 standard deviations from the mean for each row\n",
    "numeric_cols = data_cleaned.select_dtypes(include='number').columns\n",
    "for idx, row in data_cleaned[numeric_cols].iterrows():\n",
    "    row_mean = row.mean()\n",
    "    row_std_dev = row.std()\n",
    "    outlier_threshold = 4 * row_std_dev\n",
    "\n",
    "    # Check if any value in the row is an outlier\n",
    "    if any((row > row_mean + outlier_threshold) | (row < row_mean - outlier_threshold)):\n",
    "        rows_to_remove.add(idx)\n",
    "\n",
    "outliers_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices]\n",
    "outliers_count = len(outliers_indices)\n",
    "\n",
    "# Step 4: Remove Rows with All Zeros (for numeric columns) and add them to rows_to_remove\n",
    "zero_rows = data_cleaned[numeric_cols].eq(0).all(axis=1)\n",
    "rows_to_remove.update(data_cleaned[zero_rows].index)\n",
    "all_zeros_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices and idx not in outliers_indices]\n",
    "all_zeros_count = len(all_zeros_indices)\n",
    "\n",
    "# Step 5: Remove Duplicate Records\n",
    "# Find and add duplicate rows to rows_to_remove\n",
    "duplicates = data_cleaned[data_cleaned.duplicated(keep=False)].index\n",
    "rows_to_remove.update(duplicates)\n",
    "duplicates_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices and idx not in outliers_indices and idx not in all_zeros_indices]\n",
    "duplicates_count = len(duplicates_indices)\n",
    "\n",
    "# Convert rows_to_remove to a list and sort it\n",
    "rows_to_remove = sorted(list(rows_to_remove))\n",
    "\n",
    "# Remove all identified rows from data_cleaned\n",
    "data_cleaned = data_cleaned.drop(index=rows_to_remove, errors='ignore')\n",
    "\n",
    "# Extract the removed rows from the original dataset\n",
    "removed_rows = data.loc[rows_to_remove]\n",
    "\n",
    "# Convert the removed rows to a numpy matrix\n",
    "removed_rows_matrix = removed_rows.to_numpy()\n",
    "\n",
    "# Save the cleaned data and removed rows into new Excel files\n",
    "cleaned_file_path = 'cleaned_reshaped_merged_8789.xlsx'\n",
    "removed_rows_file_path = 'removed_rows_reshaped_merged_8789.xlsx'\n",
    "\n",
    "# Save the cleaned data to an Excel file\n",
    "data_cleaned.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Save the removed rows to a separate Excel file\n",
    "removed_rows.to_excel(removed_rows_file_path, index=False)\n",
    "\n",
    "# Output the file paths and counts for user's reference\n",
    "print(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "print(f\"Removed rows saved to: {removed_rows_file_path}\")\n",
    "\n",
    "# Output each step's removal details\n",
    "print(f\"Number of rows removed due to missing values: {missing_values_count}, Rows: {missing_values_indices}\")\n",
    "print(f\"Number of rows removed due to negative values: {negative_values_count}, Rows: {negative_values_indices}\")\n",
    "print(f\"Number of rows removed due to outliers: {outliers_count}, Rows: {outliers_indices}\")\n",
    "print(f\"Number of rows removed due to all-zero values: {all_zeros_count}, Rows: {all_zeros_indices}\")\n",
    "print(f\"Number of rows removed due to duplicates: {duplicates_count}, Rows: {duplicates_indices}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning_for_all_files_in_one_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the input and output directories using the current directory\n",
    "current_dir = os.getcwd()  \n",
    "input_dir = current_dir \n",
    "output_cleaned_dir = os.path.join(current_dir, 'cleaned_files')  \n",
    "output_removed_dir = os.path.join(current_dir, 'removed_files')  \n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_cleaned_dir, exist_ok=True)\n",
    "os.makedirs(output_removed_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Load the dataset\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Step 1: Handle Missing Values\n",
    "        data_cleaned = data.copy()\n",
    "        rows_to_remove = set(data_cleaned[data_cleaned.isna().any(axis=1)].index)\n",
    "        missing_values_indices = list(rows_to_remove)\n",
    "        missing_values_count = len(missing_values_indices)\n",
    "\n",
    "        # Step 2: Remove Rows with Negative Values\n",
    "        for col in data_cleaned.select_dtypes(include='number').columns:\n",
    "            negative_indices = data_cleaned[data_cleaned[col] < 0].index\n",
    "            rows_to_remove.update(negative_indices)\n",
    "        negative_values_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices]\n",
    "        negative_values_count = len(negative_values_indices)\n",
    "\n",
    "        # Step 3: Handle Outliers by Removing Rows with Outliers (Based on Row Statistics)\n",
    "        numeric_cols = data_cleaned.select_dtypes(include='number').columns\n",
    "        for idx, row in data_cleaned[numeric_cols].iterrows():\n",
    "            row_mean = row.mean()\n",
    "            row_std_dev = row.std()\n",
    "            outlier_threshold = 4 * row_std_dev\n",
    "\n",
    "            # Check if any value in the row is an outlier\n",
    "            if any((row > row_mean + outlier_threshold) | (row < row_mean - outlier_threshold)):\n",
    "                rows_to_remove.add(idx)\n",
    "\n",
    "        outliers_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices]\n",
    "        outliers_count = len(outliers_indices)\n",
    "\n",
    "\n",
    "        # Step 4: Remove Rows with 16 or more zeros (for numeric columns)\n",
    "        zero_count_per_row = data_cleaned[numeric_cols].eq(0).sum(axis=1)  \n",
    "        rows_to_remove.update(zero_count_per_row[zero_count_per_row >= 12].index)   \n",
    "        all_zeros_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices and idx not in outliers_indices]\n",
    "        all_zeros_count = len(all_zeros_indices)\n",
    "\n",
    "\n",
    "        # Step 5: Remove Duplicate Records\n",
    "        duplicates = data_cleaned[data_cleaned.duplicated(keep=False)].index\n",
    "        rows_to_remove.update(duplicates)\n",
    "        duplicates_indices = [idx for idx in rows_to_remove if idx not in missing_values_indices and idx not in negative_values_indices and idx not in outliers_indices and idx not in all_zeros_indices]\n",
    "        duplicates_count = len(duplicates_indices)\n",
    "\n",
    "        # Convert rows_to_remove to a list and sort it\n",
    "        rows_to_remove = sorted(list(rows_to_remove))\n",
    "\n",
    "        # Remove all identified rows from data_cleaned\n",
    "        data_cleaned = data_cleaned.drop(index=rows_to_remove, errors='ignore')\n",
    "\n",
    "        # Extract the removed rows from the original dataset\n",
    "        removed_rows = data.loc[rows_to_remove]\n",
    "\n",
    "        # Save the cleaned data and removed rows into new Excel files\n",
    "        cleaned_file_path = os.path.join(output_cleaned_dir, f'cleaned_{filename.replace(\".csv\", \".xlsx\")}')\n",
    "        removed_rows_file_path = os.path.join(output_removed_dir, f'removed_{filename.replace(\".csv\", \".xlsx\")}')\n",
    "\n",
    "        # Save the cleaned data to an Excel file\n",
    "        data_cleaned.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "        # Save the removed rows to a separate Excel file\n",
    "        removed_rows.to_excel(removed_rows_file_path, index=False)\n",
    "\n",
    "        # Output each step's removal details for the current file\n",
    "        print(f\"Processed file: {filename}\")\n",
    "        print(f\"Number of rows removed due to missing values: {missing_values_count}, Rows: {missing_values_indices}\")\n",
    "        print(f\"Number of rows removed due to negative values: {negative_values_count}, Rows: {negative_values_indices}\")\n",
    "        print(f\"Number of rows removed due to outliers: {outliers_count}, Rows: {outliers_indices}\")\n",
    "        print(f\"Number of rows removed due to all-zero values: {all_zeros_count}, Rows: {all_zeros_indices}\")\n",
    "        print(f\"Number of rows removed due to duplicates: {duplicates_count}, Rows: {duplicates_indices}\")\n",
    "        print(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "        print(f\"Removed rows saved to: {removed_rows_file_path}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a little check for which rows were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the set of removed row indices to a list\n",
    "removed_rows_indices = list(rows_to_remove)\n",
    "\n",
    "# Find the removed rows from the original dataset\n",
    "removed_rows = data.loc[removed_rows_indices]\n",
    "\n",
    "# Convert the removed rows to a numpy matrix\n",
    "removed_rows_matrix = removed_rows.to_numpy()\n",
    "\n",
    "# Display the shape of the numpy matrix to understand the size\n",
    "removed_rows_matrix_shape = removed_rows_matrix.shape\n",
    "\n",
    "# Output the result for user's reference\n",
    "removed_rows_matrix_shape, removed_rows_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merge All buildings in one sheet and Uniform format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the input files (use current directory)\n",
    "input_dir = os.getcwd()  # 当前文件夹路径\n",
    "output_path = 'demo_form_recording.xlsx'\n",
    "\n",
    "# Get a list of all Excel files in the directory\n",
    "file_paths = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.xlsx')]\n",
    "\n",
    "# Load all files into DataFrames and add a \"Building\" column to track the origin of each row\n",
    "dfs = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Extract the building name from the file name\n",
    "    building_name = os.path.basename(file_path).replace('.xlsx', '')\n",
    "    # Add a column to identify the building each row came from\n",
    "    df['Building'] = building_name\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all dataframes into one for processing\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extract the year and month from the 'Date' column and add 'year' and 'month' columns\n",
    "combined_df['year'] = pd.to_datetime(combined_df['Date']).dt.year\n",
    "combined_df['month'] = pd.to_datetime(combined_df['Date']).dt.month\n",
    "\n",
    "# Extract unique years and months from the dataset\n",
    "unique_years = sorted(combined_df['year'].unique())\n",
    "unique_months = sorted(combined_df['month'].unique())\n",
    "\n",
    "# Save the formatted results into a new Excel file with separate sheets for each year and month\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    for year in unique_years:\n",
    "        for month in unique_months:\n",
    "            # Filter the combined dataframe for the current year and month\n",
    "            df_filtered = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)].drop(columns=['year', 'month'])\n",
    "\n",
    "            # Define sheet name using year and month\n",
    "            sheet_name = f'{year}_Month_{month}'\n",
    "            \n",
    "            # Write the formatted dataframe to a new sheet named after the year and month\n",
    "            if not df_filtered.empty:  # Only write non-empty dataframes\n",
    "                df_filtered.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "\n",
    "print(f\"Formatted data saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
